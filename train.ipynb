{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install pandas tqdm torch scikit-learn\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import models\n",
    "from utils import save_model, load_model, upload_models_hf\n",
    "from data.dataset import PatientDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"dataset/reduced/train\"\n",
    "labels_path = \"dataset/reduced/train.csv\"\n",
    "batch_size = 1\n",
    "model_type = \"cnn\"\n",
    "cuda = True\n",
    "lr = 1e-3\n",
    "epochs = 100\n",
    "depth = 64 # number of CT slices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by filtering out the dataframe for any patients where we are missing CT scans for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2886, 6)\n"
     ]
    }
   ],
   "source": [
    "entries = os.listdir(images_path)\n",
    "patient_ids = [int(entry) for entry in entries if os.path.isdir(os.path.join(images_path, entry))]\n",
    "\n",
    "df = pd.read_csv(labels_path)\n",
    "df = df[df['patient_id'].isin(patient_ids)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we perform a standard train-validation-test split of 80-10-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2308, 6)\n",
      "(289, 6)\n",
      "(289, 6)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train, validation, and test sets\n",
    "train_data, val_test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "val_data, test_data = train_test_split(val_test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(val_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patient_id</th>\n",
       "      <th>bowel</th>\n",
       "      <th>extravastion</th>\n",
       "      <th>kidney</th>\n",
       "      <th>liver</th>\n",
       "      <th>spleen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>38343</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>964</th>\n",
       "      <td>29412</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2608</th>\n",
       "      <td>63193</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>662</th>\n",
       "      <td>23709</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2408</th>\n",
       "      <td>58547</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      patient_id  bowel  extravastion  kidney  liver  spleen\n",
       "1426       38343      0             1       0      0       1\n",
       "964        29412      0             0       0      0       0\n",
       "2608       63193      0             0       0      0       0\n",
       "662        23709      0             0       0      0       0\n",
       "2408       58547      0             0       0      0       0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeding this data into the pytorch dataset object we now have our dataloaders ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 128, 128])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PatientDataset(images_path, train_data, n_slices=depth)\n",
    "val_dataset = PatientDataset(images_path, val_data, n_slices=depth)\n",
    "test_dataset = PatientDataset(images_path, test_data, n_slices=depth)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# get one sample\n",
    "inputs, labels = train_dataset[0]\n",
    "\n",
    "print(inputs.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"cnn\":\n",
    "    model = models.ConvNet3D(\n",
    "        in_channels=inputs.shape[0],\n",
    "        out_channels=labels.shape[0],\n",
    "        depth=inputs.shape[1],\n",
    "        height=inputs.shape[2],\n",
    "        width=inputs.shape[3],\n",
    "    )\n",
    "elif model_type == \"unet\":\n",
    "    model = ...\n",
    "else:\n",
    "    raise ValueError(\"Invalid model selected for training.\")\n",
    "\n",
    "if cuda:\n",
    "    model = model.cuda()\n",
    "\n",
    "# test save and loader functions\n",
    "os.makedirs(\"checkpoints\", exist_ok = True) # create ckpt dir\n",
    "save_model(model, \"checkpoints/test.pth\")\n",
    "load_model(model, \"checkpoints/test.pth\")\n",
    "os.remove(\"checkpoints/test.pth\")\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()  # multi-label classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() # set model to train\n",
    "\n",
    "    # creating loss tracker\n",
    "    running_loss = 0.0\n",
    "    running_vloss = 0.0\n",
    "    batch_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n",
    "    for i, (inputs, labels) in pbar:\n",
    "        if cuda:\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # zero gradients for every batch\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # compute predictions + loss\n",
    "        outputs = model(inputs.float())\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # perform backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        batch_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            batch_loss = batch_loss / 10 # loss per batch\n",
    "            pbar.set_postfix({'loss': round(batch_loss, 5)})\n",
    "            batch_loss = 0.0\n",
    "\n",
    "    avg_loss = running_loss / (i + 1)\n",
    "    train_losses.append((epoch, avg_loss))\n",
    "\n",
    "    print(\"Validation...\")\n",
    "    model.eval() # set model to evaluation\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in val_dataloader:\n",
    "            if cuda:\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "                \n",
    "            outputs = model(inputs.float())\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_vloss += loss.item()\n",
    "\n",
    "    avg_vloss = running_vloss / len(val_dataloader)\n",
    "    val_losses.append((epoch, avg_vloss))\n",
    "\n",
    "    print('[LOSS] train {} val {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # save model when\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        save_model(model, f\"checkpoints/BEST_model_{timestamp}_{epoch}.pth\")\n",
    "\n",
    "    if epoch != 0 and epoch % 20 == 0:\n",
    "        save_model(model, f\"checkpoints/model_{timestamp}_{epoch}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the training and validation loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_iters, t_loss = list(zip(*train_losses))\n",
    "plt.title(\"Training Curve (batch_size={}, lr={})\".format(batch_size, lr))\n",
    "plt.plot(t_iters, t_loss, label=\"Train\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()\n",
    "\n",
    "v_iters, v_loss = list(zip(*val_losses))\n",
    "plt.title(\"Validation Curve (batch_size={}, lr={})\".format(batch_size, lr))\n",
    "plt.plot(v_iters, v_loss, label=\"Validation\")\n",
    "plt.xlabel(\"Validation Epochs\")    \n",
    "plt.ylabel(\"Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_models_hf(\"checkpoints/\", \"Pie31415\", \"abd-trauma-models\") # upload to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, inputs):\n",
    "    with torch.no_grad():\n",
    "        outputs = model(inputs)\n",
    "        pred = (outputs > 0.5).int()\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
